{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laguage Translator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROSpFomxl_N"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from string import digits\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkWgX0TbFJDS"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "2Y6AqDzxDExg",
        "outputId": "7b2beea3-4564-4948-d2f1-57aaac97c0ca"
      },
      "source": [
        "#load text file data into Dataframe\n",
        "data = pd.read_table('french.txt', names=[\"English\", \"French\"])\n",
        "\n",
        "SIZE = 10000 #@param{type:'slider', min:1000, max:149861, step:1000}\n",
        "data = data[:SIZE]\n",
        "#display random rows\n",
        "data.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8564</th>\n",
              "      <td>We must escape.</td>\n",
              "      <td>Il nous faut nous échapper.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4668</th>\n",
              "      <td>Have a cookie.</td>\n",
              "      <td>Prends un biscuit !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5350</th>\n",
              "      <td>I'm depressed.</td>\n",
              "      <td>Je suis déprimé.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7887</th>\n",
              "      <td>It's not right.</td>\n",
              "      <td>Ce n'est pas exact.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>I feel cold.</td>\n",
              "      <td>J'ai froid.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              English                       French\n",
              "8564  We must escape.  Il nous faut nous échapper.\n",
              "4668   Have a cookie.          Prends un biscuit !\n",
              "5350   I'm depressed.             Je suis déprimé.\n",
              "7887  It's not right.          Ce n'est pas exact.\n",
              "1987     I feel cold.                  J'ai froid."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZRaUug0Ef2l",
        "outputId": "bd9c434c-7cf5-4f27-b751-938404486a7e"
      },
      "source": [
        "#lets check out total number of sentences/rows in data\n",
        "len(data) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYRPGs2GFL2S"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIyPWwYJE5Sr"
      },
      "source": [
        "#convert all sentences into lower case.\n",
        "data.English = data.English.apply(lambda x : x.lower())\n",
        "data.French = data.French.apply(lambda x : x.lower())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BnFOddtGfBj"
      },
      "source": [
        "#remove punctuation from sentences\n",
        "punctuations = set(string.punctuation)\n",
        "data.English = data.English.apply(lambda x: \"\".join(letter for letter in x if letter not in punctuations))\n",
        "data.French = data.French.apply(lambda x: \"\".join(letter for letter in x if letter not in punctuations))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbYjptCvGr5J"
      },
      "source": [
        "#convert all digits into None from sentences\n",
        "digit_remover = str.maketrans('','',digits)\n",
        "data.English = data.English.apply(lambda x : x.translate(digit_remover))\n",
        "data.French = data.French.apply(lambda x : x.translate(digit_remover))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "wKtl1MB_Ll-I",
        "outputId": "31859be1-34b3-4396-c027-fe41c0a77516"
      },
      "source": [
        "data.sample(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7280</th>\n",
              "      <td>i left my wife</td>\n",
              "      <td>jai quitté ma femme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>629</th>\n",
              "      <td>im armed</td>\n",
              "      <td>je suis armée</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4567</th>\n",
              "      <td>do you see me</td>\n",
              "      <td>me voyezvous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1104</th>\n",
              "      <td>i can jump</td>\n",
              "      <td>je peux sauter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8373</th>\n",
              "      <td>this is a fact</td>\n",
              "      <td>cest un fait</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             English               French\n",
              "7280  i left my wife  jai quitté ma femme\n",
              "629         im armed        je suis armée\n",
              "4567   do you see me        me voyezvous \n",
              "1104      i can jump       je peux sauter\n",
              "8373  this is a fact         cest un fait"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPr7uHG5L24g"
      },
      "source": [
        "#Apply \"START\" & \"END\" token in french sentences\n",
        "data.French = data.French.apply(lambda x : \"START\"+ \" \" + x +\" \" +\"END\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "45r-aFbyMqOJ",
        "outputId": "63e61ca3-b44c-4d7d-f17d-75d2d06cb271"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>go</td>\n",
              "      <td>START va  END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>run</td>\n",
              "      <td>START cours  END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>run</td>\n",
              "      <td>START courez  END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wow</td>\n",
              "      <td>START ça alors  END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fire</td>\n",
              "      <td>START au feu  END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English               French\n",
              "0      go        START va  END\n",
              "1     run     START cours  END\n",
              "2     run    START courez  END\n",
              "3     wow  START ça alors  END\n",
              "4    fire    START au feu  END"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsfexHdkbSNG"
      },
      "source": [
        "english_sentences = data.English.values\n",
        "french_sentences = data.French.values\n",
        "\n",
        "#Define tokenizer\n",
        "encoder_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "decoder_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "\n",
        "#fit all sentences into tokenizer\n",
        "encoder_tokenizer.fit_on_texts(english_sentences)\n",
        "decoder_tokenizer.fit_on_texts(french_sentences)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXJRzXX2cF1-"
      },
      "source": [
        "#get word indeces\n",
        "encoder_word_index = encoder_tokenizer.word_index\n",
        "decoder_word_index = decoder_tokenizer.word_index"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYz-OOqAcG-d"
      },
      "source": [
        "#convert sentences into sequences\n",
        "encoder_sequences = encoder_tokenizer.texts_to_sequences(english_sentences)\n",
        "decoder_sequences = decoder_tokenizer.texts_to_sequences(french_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8NF4IJ5fLHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b56af8-a6f2-4a87-e32d-0c49d31dd98b"
      },
      "source": [
        "#lets find max length of sentences\n",
        "lengths = list()\n",
        "for sentence in english_sentences:\n",
        "  lengths.append(len(sentence.split()))\n",
        "max_encoder_length=np.max(lengths)\n",
        "\n",
        "lengths = list()\n",
        "for sentence in french_sentences:\n",
        "  lengths.append(len(sentence.split()))\n",
        "max_decoder_length=np.max(lengths)\n",
        "\n",
        "print(\"Max encoder length => \",max_encoder_length)\n",
        "print(\"Max decoder length => \",max_decoder_length)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max encoder length =>  5\n",
            "Max decoder length =>  12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoCjCWRVdnrV"
      },
      "source": [
        "#pad sequences with 0s\n",
        "encoder_input_data = pad_sequences(encoder_sequences, padding='post')\n",
        "decoder_input_data = pad_sequences(decoder_sequences, padding='post')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF--BKG_dMa_",
        "outputId": "7e995c63-451c-4fa0-c1b0-eb092177c5dc"
      },
      "source": [
        "#display any sentence and corresponding sequence and padded sequence\n",
        "print(\"sentence : \", english_sentences[1000])\n",
        "print('sequence : ', encoder_sequences[1000])\n",
        "print('sequence : ', encoder_input_data[1000])\n",
        "print(\"default length of sequence : \", len(encoder_input_data[1000]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence :  come alone\n",
            "sequence :  [37, 96]\n",
            "sequence :  [37 96  0  0  0]\n",
            "default length of sequence :  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om0GPtU-oHdx"
      },
      "source": [
        "decoder_output_data = []\n",
        "for seq in decoder_input_data:\n",
        "  temp = []\n",
        "  for number in seq: \n",
        "    one_hot = np.zeros(len(decoder_word_index), dtype='float32')\n",
        "    if number!=0:\n",
        "      one_hot[number-1] = float(1)\n",
        "    temp.append(one_hot)\n",
        "  decoder_output_data.append(temp)\n",
        "decoder_output_data = np.array(decoder_output_data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP-4fZUs0wBM",
        "outputId": "350bd2ae-52ba-4f05-aa88-5519915a2103"
      },
      "source": [
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)\n",
        "print(decoder_output_data.shape)\n",
        "print(len(encoder_word_index))\n",
        "print(len(decoder_word_index))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 5)\n",
            "(10000, 12)\n",
            "(10000, 12, 4837)\n",
            "2146\n",
            "4837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VvaWX8Zh93N",
        "outputId": "9d5562d0-c22f-4bdc-f220-adbede1f6832"
      },
      "source": [
        "#display any sentence and corresponding sequence and padded sequence\n",
        "print(\"sentence : \", french_sentences[1000])\n",
        "print('sequence : ', decoder_sequences[1000])\n",
        "print('sequence : ', decoder_input_data[1000])\n",
        "print(\"default length of sequence : \", len(decoder_input_data[1000]))\n",
        "print(\"one-hot of sequence : \", decoder_output_data[1000])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence :  START venez seule  END\n",
            "sequence :  [2, 146, 323, 3]\n",
            "sequence :  [  2 146 323   3   0   0   0   0   0   0   0   0]\n",
            "default length of sequence :  12\n",
            "one-hot of sequence :  [[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kp1mihzp-_r"
      },
      "source": [
        "### Model Development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrSxy3xno80s"
      },
      "source": [
        "#hyperparameters\n",
        "embedding_size = 120\n",
        "lstm_dim = 364\n",
        "\n",
        "#parameters\n",
        "encoder_vocab_size = len(encoder_word_index)\n",
        "decoder_vocab_size = len(decoder_word_index)\n",
        "#input_shape = (max_encoder_length,)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_EXyjw5qKkF"
      },
      "source": [
        "#building model \n",
        "#Encoder model\n",
        "encoder_input = Input(shape=(None,))\n",
        "E_x = Embedding(encoder_vocab_size+1, embedding_size)(encoder_input)\n",
        "encoder = LSTM(lstm_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(E_x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "#decoder model\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder_E_x = Embedding(decoder_vocab_size+1, embedding_size)(decoder_input)\n",
        "decoder_lstm = LSTM(lstm_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_E_x, initial_state=encoder_states)\n",
        "decoder_dense= Dense(decoder_vocab_size, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfLsIIiHrw-z"
      },
      "source": [
        "model = Model([encoder_input, decoder_input], decoder_outputs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv94WAs0xLeJ"
      },
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-aux3rjxXx5",
        "outputId": "05135f71-d0e3-4eed-8030-31e2be6e591c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 120)    257640      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 120)    580560      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 364),        706160      ['embedding[0][0]']              \n",
            "                                 (None, 364),                                                     \n",
            "                                 (None, 364)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 364),  706160      ['embedding_1[0][0]',            \n",
            "                                 (None, 364),                     'lstm[0][1]',                   \n",
            "                                 (None, 364)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 4837)   1765505     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,016,025\n",
            "Trainable params: 4,016,025\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pROnsc10xZRU",
        "outputId": "8fb46660-996f-435c-de2d-f6fce85eabeb"
      },
      "source": [
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=100)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "157/157 [==============================] - 15s 37ms/step - loss: 2.0750 - accuracy: 0.1711\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 1.8108 - accuracy: 0.2060\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 1.6356 - accuracy: 0.2339\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 1.4517 - accuracy: 0.2617\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 1.2204 - accuracy: 0.2894\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 1.0314 - accuracy: 0.3048\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.8919 - accuracy: 0.3180\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.7825 - accuracy: 0.3306\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.6896 - accuracy: 0.3408\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.6090 - accuracy: 0.3498\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.5401 - accuracy: 0.3571\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.4798 - accuracy: 0.3634\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.4256 - accuracy: 0.3710\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.3793 - accuracy: 0.3767\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.3397 - accuracy: 0.3825\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.3030 - accuracy: 0.3875\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.2717 - accuracy: 0.3914\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.2431 - accuracy: 0.3960\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.2186 - accuracy: 0.3995\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.1972 - accuracy: 0.4029\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.1786 - accuracy: 0.4055\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.1632 - accuracy: 0.4083\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.1482 - accuracy: 0.4106\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.1344 - accuracy: 0.4128\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.1204 - accuracy: 0.4148\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.1082 - accuracy: 0.4172\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0972 - accuracy: 0.4192\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0866 - accuracy: 0.4212\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0772 - accuracy: 0.4231\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0685 - accuracy: 0.4255\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0602 - accuracy: 0.4277\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0535 - accuracy: 0.4297\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0471 - accuracy: 0.4316\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0416 - accuracy: 0.4333\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0368 - accuracy: 0.4342\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0327 - accuracy: 0.4353\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0293 - accuracy: 0.4360\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0259 - accuracy: 0.4366\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0230 - accuracy: 0.4369\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0207 - accuracy: 0.4373\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0187 - accuracy: 0.4375\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0167 - accuracy: 0.4377\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0153 - accuracy: 0.4378\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0139 - accuracy: 0.4378\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0128 - accuracy: 0.4379\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0117 - accuracy: 0.4380\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0109 - accuracy: 0.4380\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0101 - accuracy: 0.4381\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0093 - accuracy: 0.4381\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0087 - accuracy: 0.4381\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0081 - accuracy: 0.4381\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0076 - accuracy: 0.4381\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0071 - accuracy: 0.4381\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0068 - accuracy: 0.4381\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0064 - accuracy: 0.4381\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0062 - accuracy: 0.4381\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0058 - accuracy: 0.4381\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0055 - accuracy: 0.4381\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0053 - accuracy: 0.4381\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0051 - accuracy: 0.4381\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0049 - accuracy: 0.4381\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0047 - accuracy: 0.4381\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0045 - accuracy: 0.4381\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0044 - accuracy: 0.4381\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0042 - accuracy: 0.4381\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0041 - accuracy: 0.4381\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0039 - accuracy: 0.4381\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0038 - accuracy: 0.4381\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0037 - accuracy: 0.4381\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0036 - accuracy: 0.4381\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0035 - accuracy: 0.4381\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0034 - accuracy: 0.4381\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0033 - accuracy: 0.4381\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0032 - accuracy: 0.4381\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0031 - accuracy: 0.4381\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0030 - accuracy: 0.4381\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0030 - accuracy: 0.4381\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0029 - accuracy: 0.4381\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0028 - accuracy: 0.4381\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0028 - accuracy: 0.4381\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0027 - accuracy: 0.4381\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0026 - accuracy: 0.4381\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0026 - accuracy: 0.4381\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0025 - accuracy: 0.4381\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0025 - accuracy: 0.4381\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0024 - accuracy: 0.4381\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0024 - accuracy: 0.4381\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0023 - accuracy: 0.4381\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0023 - accuracy: 0.4381\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0022 - accuracy: 0.4381\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0022 - accuracy: 0.4381\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0022 - accuracy: 0.4381\n",
            "Epoch 93/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0021 - accuracy: 0.4381\n",
            "Epoch 94/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0021 - accuracy: 0.4381\n",
            "Epoch 95/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0020 - accuracy: 0.4381\n",
            "Epoch 96/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0020 - accuracy: 0.4381\n",
            "Epoch 97/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0019 - accuracy: 0.4381\n",
            "Epoch 98/100\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0019 - accuracy: 0.4381\n",
            "Epoch 99/100\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.0019 - accuracy: 0.4381\n",
            "Epoch 100/100\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0019 - accuracy: 0.4381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "o-Avu8W8-s58",
        "outputId": "b18f6026-cefc-45f9-9906-1f96e3dd6476"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graph(history, string):\n",
        "  x = np.arange(0,100)\n",
        "  plt.plot(x, history.history[string], label=string)\n",
        "  plt.plot(x, history.history[\"val_\"+string], label=\"val_\"+string)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_graph(history, \"accuracy\")\n",
        "plot_graph(history, \"loss\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b36625dfee0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-b36625dfee0c>\u001b[0m in \u001b[0;36mplot_graph\u001b[0;34m(history, string)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q1tcld3-wNm",
        "outputId": "31c414bf-2e14-423b-9018-d5e725deb4de"
      },
      "source": [
        "#Inference Stage\n",
        "\n",
        "#encoder model\n",
        "encoder_model = Model(encoder_input, encoder_states)\n",
        "encoder_model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 120)         257640    \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 364),             706160    \n",
            "                              (None, 364),                       \n",
            "                              (None, 364)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 963,800\n",
            "Trainable params: 963,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjJd5F2gDvY9"
      },
      "source": [
        "#decoder model\n",
        "decoder_state_input_h = Input(shape=(lstm_dim,))\n",
        "decoder_state_input_c = Input(shape=(lstm_dim,))\n",
        "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "final_dex2 = Embedding(decoder_vocab_size, embedding_size)(decoder_input)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_state_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model([decoder_input] + decoder_state_inputs, [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drCDRuWhDzkd",
        "outputId": "37afb6ad-a9a4-420c-96d4-f393f9d1dc01"
      },
      "source": [
        "# reversing the word index dictionary to get words from index values\n",
        "reverse_input_char_index = dict((i,char) for char, i in encoder_word_index.items())\n",
        "reverse_output_char_index = dict((i,char) for char, i in decoder_word_index.items())\n",
        "print(reverse_input_char_index)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: '<OOV>', 2: 'i', 3: 'you', 4: 'it', 5: 'im', 6: 'a', 7: 'is', 8: 'he', 9: 'are', 10: 'me', 11: 'tom', 12: 'we', 13: 'youre', 14: 'go', 15: 'were', 16: 'its', 17: 'was', 18: 'dont', 19: 'do', 20: 'be', 21: 'to', 22: 'this', 23: 'that', 24: 'can', 25: 'not', 26: 'ill', 27: 'the', 28: 'up', 29: 'have', 30: 'they', 31: 'did', 32: 'she', 33: 'like', 34: 'here', 35: 'am', 36: 'no', 37: 'come', 38: 'get', 39: 'him', 40: 'my', 41: 'hes', 42: 'all', 43: 'in', 44: 'let', 45: 'thats', 46: 'your', 47: 'need', 48: 'got', 49: 'love', 50: 'us', 51: 'how', 52: 'stop', 53: 'well', 54: 'look', 55: 'out', 56: 'want', 57: 'who', 58: 'feel', 59: 'cant', 60: 'what', 61: 'take', 62: 'theyre', 63: 'on', 64: 'hate', 65: 'one', 66: 'toms', 67: 'know', 68: 'see', 69: 'home', 70: 'must', 71: 'so', 72: 'help', 73: 'back', 74: 'now', 75: 'please', 76: 'stay', 77: 'good', 78: 'busy', 79: 'felt', 80: 'just', 81: 'saw', 82: 'had', 83: 'lost', 84: 'too', 85: 'happy', 86: 'try', 87: 'leave', 88: 'give', 89: 'her', 90: 'lets', 91: 'them', 92: 'will', 93: 'work', 94: 'there', 95: 'keep', 96: 'alone', 97: 'wait', 98: 'hurt', 99: 'has', 100: 'very', 101: 'nice', 102: 'ive', 103: 'may', 104: 'didnt', 105: 'right', 106: 'made', 107: 'big', 108: 'drink', 109: 'down', 110: 'upset', 111: 'job', 112: 'mine', 113: 'ready', 114: 'tired', 115: 'at', 116: 'hurry', 117: 'of', 118: 'car', 119: 'run', 120: 'off', 121: 'done', 122: 'for', 123: 'eat', 124: 'careful', 125: 'left', 126: 'went', 127: 'fun', 128: 'again', 129: 'won', 130: 'call', 131: 'old', 132: 'drunk', 133: 'yours', 134: 'bad', 135: 'dog', 136: 'ask', 137: 'trust', 138: 'win', 139: 'bed', 140: 'early', 141: 'say', 142: 'naive', 143: 'safe', 144: 'shes', 145: 'sure', 146: 'new', 147: 'hard', 148: 'swim', 149: 'talk', 150: 'walk', 151: 'broke', 152: 'time', 153: 'away', 154: 'sorry', 155: 'wrong', 156: 'check', 157: 'door', 158: 'way', 159: 'over', 160: 'crazy', 161: 'hear', 162: 'open', 163: 'cold', 164: 'free', 165: 'late', 166: 'his', 167: 'said', 168: 'found', 169: 'fat', 170: 'could', 171: 'miss', 172: 'girls', 173: 'an', 174: 'ok', 175: 'drive', 176: 'grab', 177: 'sick', 178: 'whos', 179: 'sing', 180: 'remember', 181: 'funny', 182: 'sleep', 183: 'lot', 184: 'should', 185: 'still', 186: 'fine', 187: 'lazy', 188: 'came', 189: 'cry', 190: 'die', 191: 'read', 192: 'bring', 193: 'boy', 194: 'everyone', 195: 'sad', 196: 'going', 197: 'lucky', 198: 'buy', 199: 'prepared', 200: 'caught', 201: 'called', 202: 'quit', 203: 'show', 204: 'first', 205: 'quiet', 206: 'sign', 207: 'lonely', 208: 'make', 209: 'easy', 210: 'finished', 211: 'tie', 212: 'cool', 213: 'drop', 214: 'some', 215: 'watch', 216: 'forget', 217: 'hows', 218: 'angry', 219: 'mad', 220: 'liked', 221: 'kidding', 222: 'why', 223: 'loves', 224: 'eyes', 225: 'book', 226: 'wasnt', 227: 'seem', 228: 'tried', 229: 'shy', 230: 'find', 231: 'died', 232: 'sit', 233: 'knew', 234: 'follow', 235: 'luck', 236: 'mean', 237: 'where', 238: 'with', 239: 'more', 240: 'these', 241: 'tell', 242: 'use', 243: 'lie', 244: 'rich', 245: 'relaxed', 246: 'lying', 247: 'care', 248: 'turn', 249: 'whats', 250: 'start', 251: 'hired', 252: 'serious', 253: 'never', 254: 'close', 255: 'stupid', 256: 'cheated', 257: 'hand', 258: 'enjoy', 259: 'thanks', 260: 'calm', 261: 'hold', 262: 'shut', 263: 'pay', 264: 'dead', 265: 'gave', 266: 'met', 267: 'wont', 268: 'sleepy', 269: 'winning', 270: 'working', 271: 'money', 272: 'nothing', 273: 'seems', 274: 'man', 275: 'clean', 276: 'anyone', 277: 'cut', 278: 'life', 279: 'stand', 280: 'famous', 281: 'anybody', 282: 'yourself', 283: 'likes', 284: 'mary', 285: 'saved', 286: 'joke', 287: 'cat', 288: 'enough', 289: 'wake', 290: 'tall', 291: 'works', 292: 'inside', 293: 'hope', 294: 'id', 295: 'move', 296: 'shot', 297: 'break', 298: 'young', 299: 'coming', 300: 'wants', 301: 'boss', 302: 'idea', 303: 'god', 304: 'looks', 305: 'exhausted', 306: 'agree', 307: 'join', 308: 'weak', 309: 'lied', 310: 'stuck', 311: 'around', 312: 'yes', 313: 'better', 314: 'hungry', 315: 'normal', 316: 'strong', 317: 'as', 318: 'bored', 319: 'reading', 320: 'date', 321: 'quickly', 322: 'does', 323: 'great', 324: 'isnt', 325: 'fair', 326: 'ahead', 327: 'fast', 328: 'hurts', 329: 'hot', 330: 'live', 331: 'alive', 332: 'awake', 333: 'dying', 334: 'tough', 335: 'food', 336: 'cried', 337: 'knows', 338: 'fired', 339: 'wine', 340: 'guess', 341: 'french', 342: 'liar', 343: 'joking', 344: 'day', 345: 'merciful', 346: 'seat', 347: 'beer', 348: 'needs', 349: 'crying', 350: 'fined', 351: 'fish', 352: 'cats', 353: 'excited', 354: 'talking', 355: 'nuts', 356: 'rude', 357: 'myself', 358: 'drank', 359: 'afraid', 360: 'needed', 361: 'sent', 362: 'told', 363: 'think', 364: 'invited', 365: 'touch', 366: 'room', 367: 'fell', 368: 'awesome', 369: 'kiss', 370: 'cute', 371: 'deaf', 372: 'speak', 373: 'after', 374: 'true', 375: 'smart', 376: 'skinny', 377: 'ignore', 378: 'sweet', 379: 'looked', 380: 'punctual', 381: 'asleep', 382: 'set', 383: 'driving', 384: 'married', 385: 'trapped', 386: 'laughed', 387: 'best', 388: 'doctor', 389: 'silly', 390: 'scared', 391: 'cruel', 392: 'gear', 393: 'son', 394: 'shoe', 395: 'crafty', 396: 'arent', 397: 'much', 398: 'hat', 399: 'envious', 400: 'fit', 401: 'save', 402: 'awful', 403: 'woke', 404: 'cook', 405: 'obey', 406: 'tea', 407: 'two', 408: 'sneaky', 409: 'itll', 410: 'weird', 411: 'pick', 412: 'smell', 413: 'decide', 414: 'smoke', 415: 'both', 416: 'kids', 417: 'certain', 418: 'jealous', 419: 'stunned', 420: 'thirsty', 421: 'unlucky', 422: 'took', 423: 'dumped', 424: 'ate', 425: 'innocent', 426: 'fake', 427: 'running', 428: 'cops', 429: 'today', 430: 'welcome', 431: 'push', 432: 'and', 433: 'about', 434: 'boring', 435: 'lock', 436: 'betrayed', 437: 'hug', 438: 'hang', 439: 'hit', 440: 'forgot', 441: 'catch', 442: 'dogs', 443: 'armed', 444: 'blind', 445: 'hers', 446: 'step', 447: 'loved', 448: 'ran', 449: 'loaded', 450: 'shaken', 451: 'mind', 452: 'once', 453: 'gone', 454: 'bit', 455: 'youve', 456: 'creative', 457: 'fight', 458: 'dressed', 459: 'burned', 460: 'dance', 461: 'messed', 462: 'change', 463: 'finicky', 464: 'starved', 465: 'touched', 466: 'lunch', 467: 'nobody', 468: 'rest', 469: 'dope', 470: 'escaped', 471: 'fix', 472: 'sue', 473: 'guilty', 474: 'by', 475: 'key', 476: 'foolish', 477: 'tricked', 478: 'coffee', 479: 'outrank', 480: 'rescued', 481: 'treat', 482: 'boat', 483: 'word', 484: 'listen', 485: 'perfect', 486: 'bald', 487: 'thin', 488: 'ugly', 489: 'warn', 490: 'write', 491: 'worked', 492: 'ours', 493: 'talked', 494: 'men', 495: 'worry', 496: 'shoot', 497: 'honest', 498: 'losing', 499: 'problem', 500: 'study', 501: 'bus', 502: 'cheat', 503: 'tomll', 504: 'ruthless', 505: 'smile', 506: 'through', 507: 'song', 508: 'trying', 509: 'many', 510: 'hugged', 511: 'confident', 512: 'outside', 513: 'resist', 514: 'gun', 515: 'school', 516: 'doubts', 517: 'music', 518: 'water', 519: 'lose', 520: 'started', 521: 'divorced', 522: 'speaking', 523: 'upstairs', 524: 'seen', 525: 'shame', 526: 'yelling', 527: 'youll', 528: 'comes', 529: 'fed', 530: 'wife', 531: 'walked', 532: 'wanted', 533: 'clever', 534: 'trusts', 535: 'weve', 536: 'rid', 537: 'believed', 538: 'followed', 539: 'fire', 540: 'jump', 541: 'oh', 542: 'attack', 543: 'really', 544: 'kind', 545: 'slow', 546: 'real', 547: 'guys', 548: 'failed', 549: 'helps', 550: 'answer', 551: 'beg', 552: 'ski', 553: 'fussy', 554: 'cares', 555: 'clear', 556: 'content', 557: 'quick', 558: 'duty', 559: 'built', 560: 'promised', 561: 'eating', 562: 'greedy', 563: 'ruined', 564: 'smiled', 565: 'aside', 566: 'helped', 567: 'discreet', 568: 'friendly', 569: 'hated', 570: 'curious', 571: 'dumb', 572: 'milk', 573: 'cars', 574: 'moved', 575: 'manage', 576: 'ashamed', 577: 'dieting', 578: 'patient', 579: 'psychic', 580: 'doll', 581: 'list', 582: 'wish', 583: 'tight', 584: 'changed', 585: 'kid', 586: 'brave', 587: 'proof', 588: 'heard', 589: 'missed', 590: 'play', 591: 'addicted', 592: 'confused', 593: 'stubborn', 594: 'our', 595: 'looking', 596: 'hates', 597: 'ignored', 598: 'thief', 599: 'smells', 600: 'dinner', 601: 'later', 602: 'kept', 603: 'smashed', 604: 'bright', 605: 'grumpy', 606: 'annoy', 607: 'guy', 608: 'doors', 609: 'wet', 610: 'wash', 611: 'glad', 612: 'paid', 613: 'bless', 614: 'soon', 615: 'heres', 616: 'pack', 617: 'stinks', 618: 'loosen', 619: 'course', 620: 'agreed', 621: 'waited', 622: 'lovely', 623: 'hero', 624: 'single', 625: 'blue', 626: 'night', 627: 'relax', 628: 'cover', 629: 'taste', 630: 'deal', 631: 'laugh', 632: 'grew', 633: 'also', 634: 'apologize', 635: 'asked', 636: 'bought', 637: 'fruit', 638: 'dizzy', 639: 'cash', 640: 'bet', 641: 'often', 642: 'eggs', 643: 'correct', 644: 'nervous', 645: 'retired', 646: 'sloshed', 647: 'broken', 648: 'timing', 649: 'grow', 650: 'put', 651: 'bite', 652: 'slowly', 653: 'pain', 654: 'team', 655: 'wheres', 656: 'fall', 657: 'pen', 658: 'believe', 659: 'faint', 660: 'handled', 661: 'map', 662: 'shoes', 663: 'warned', 664: 'twice', 665: 'explain', 666: 'kill', 667: 'risk', 668: 'lawyer', 669: 'outraged', 670: 'powerful', 671: 'sleeping', 672: 'starving', 673: 'law', 674: 'singing', 675: 'closely', 676: 'someone', 677: 'smoking', 678: 'small', 679: 'respectful', 680: 'dream', 681: 'nose', 682: 'soccer', 683: 'boston', 684: 'own', 685: 'volunteered', 686: 'teacher', 687: 'short', 688: 'last', 689: 'suits', 690: 'plan', 691: 'six', 692: 'friends', 693: 'when', 694: 'insane', 695: 'pretty', 696: 'girl', 697: 'friend', 698: 'cookies', 699: 'handle', 700: 'used', 701: 'from', 702: 'those', 703: 'precise', 704: 'cheers', 705: 'beat', 706: 'long', 707: 'refuse', 708: 'stayed', 709: 'marry', 710: 'terrific', 711: 'lies', 712: 'noticed', 713: 'far', 714: 'dark', 715: 'hello', 716: 'walks', 717: 'swam', 718: 'spoke', 719: 'fret', 720: 'forgive', 721: 'poor', 722: 'envy', 723: 'fixed', 724: 'air', 725: 'survived', 726: 'wrote', 727: 'chubby', 728: 'sharp', 729: 'drinks', 730: 'obeyed', 731: 'even', 732: 'watchful', 733: 'count', 734: 'box', 735: 'stood', 736: 'recovered', 737: 'soup', 738: 'cancel', 739: 'blessed', 740: 'cooking', 741: 'dancing', 742: 'leaving', 743: 'fool', 744: 'selfish', 745: 'shocked', 746: 'sincere', 747: 'staying', 748: 'stuffed', 749: 'unhappy', 750: 'dirty', 751: 'memorize', 752: 'card', 753: 'shall', 754: 'kissed', 755: 'war', 756: 'act', 757: 'twins', 758: 'mess', 759: 'whose', 760: 'attentive', 761: 'rifle', 762: 'donut', 763: 'denied', 764: 'face', 765: 'english', 766: 'bigot', 767: 'always', 768: 'giddy', 769: 'naked', 770: 'plans', 771: 'farmer', 772: 'saint', 773: 'offended', 774: 'pregnant', 775: 'thorough', 776: 'nearby', 777: 'unfair', 778: 'black', 779: 'writing', 780: 'pigs', 781: 'feet', 782: 'sued', 783: 'arguing', 784: 'overslept', 785: 'biased', 786: 'dreaming', 787: 'any', 788: 'pitch', 789: 'draw', 790: 'dry', 791: 'feed', 792: 'bird', 793: 'killed', 794: 'studying', 795: 'beautiful', 796: 'already', 797: 'american', 798: 'japanese', 799: 'became', 800: 'survive', 801: 'canceled', 802: 'freaked', 803: 'clue', 804: 'winter', 805: 'hiking', 806: 'object', 807: 'owe', 808: 'horse', 809: 'slept', 810: 'trusted', 811: 'prove', 812: 'student', 813: 'artist', 814: 'forgetful', 815: 'fan', 816: 'observant', 817: 'sense', 818: 'outdated', 819: 'minute', 820: 'number', 821: 'clearly', 822: 'moment', 823: 'positive', 824: 'arrived', 825: 'packing', 826: 'till', 827: 'creepy', 828: 'hiring', 829: 'camera', 830: 'cows', 831: 'tomorrow', 832: 'tempt', 833: 'bag', 834: 'everybody', 835: 'van', 836: 'almost', 837: 'lay', 838: 'surrendered', 839: 'wide', 840: 'arm', 841: 'honored', 842: 'tv', 843: 'turned', 844: 'doing', 845: 'undressing', 846: 'remarried', 847: 'if', 848: 'only', 849: 'redundant', 850: 'pull', 851: 'plays', 852: 'snap', 853: 'screaming', 854: 'amazing', 855: 'struggled', 856: 'prude', 857: 'congratulations', 858: 'expect', 859: 'mercy', 860: 'towel', 861: 'store', 862: 'began', 863: 'mentioned', 864: 'children', 865: 'goodbye', 866: 'runs', 867: 'hi', 868: 'deep', 869: 'phoned', 870: 'full', 871: 'okay', 872: 'odd', 873: 'birds', 874: 'cringed', 875: 'promise', 876: 'cop', 877: 'sand', 878: 'pardon', 879: 'seriously', 880: 'else', 881: 'idiot', 882: 'aboard', 883: 'warm', 884: 'hung', 885: 'disagree', 886: 'cpr', 887: 'ice', 888: 'korean', 889: 'buying', 890: 'hiding', 891: 'moving', 892: 'paying', 893: 'pooped', 894: 'matters', 895: 'fad', 896: 'green', 897: 'cd', 898: 'voted', 899: 'flies', 900: 'fail', 901: 'bother', 902: 'sensible', 903: 'finish', 904: 'jerk', 905: 'age', 906: 'confessed', 907: 'bread', 908: 'fear', 909: 'golf', 910: 'rice', 911: 'nailed', 912: 'sell', 913: 'third', 914: 'attend', 915: 'italian', 916: 'chicken', 917: 'engaged', 918: 'fasting', 919: 'healthy', 920: 'psyched', 921: 'wealthy', 922: 'worried', 923: 'legal', 924: 'happened', 925: 'secret', 926: 'begin', 927: 'party', 928: 'talks', 929: 'fainted', 930: 'fox', 931: 'lame', 932: 'wise', 933: 'refused', 934: 'sweated', 935: 'boys', 936: 'pity', 937: 'next', 938: 'anything', 939: 'vote', 940: 'cranky', 941: 'admire', 942: 'apologized', 943: 'apples', 944: 'deserve', 945: 'books', 946: 'might', 947: 'paint', 948: 'lion', 949: 'teach', 950: 'thought', 951: 'risks', 952: 'understand', 953: 'absent', 954: 'beaten', 955: 'learn', 956: 'wonder', 957: 'canadian', 958: 'ears', 959: 'drowning', 960: 'fighting', 961: 'paris', 962: 'shooting', 963: 'wolf', 964: 'happen', 965: 'garbage', 966: 'genuine', 967: 'obvious', 968: 'snowing', 969: 'meat', 970: 'while', 971: 'dreams', 972: 'passed', 973: 'aint', 974: 'crime', 975: 'blow', 976: 'deny', 977: 'circle', 978: 'something', 979: 'feels', 980: 'fooled', 981: 'type', 982: 'pale', 983: 'sells', 984: 'senior', 985: 'adorable', 986: 'tokyo', 987: 'muslim', 988: 'diabetic', 989: 'belong', 990: 'deserved', 991: 'despise', 992: 'ring', 993: 'movies', 994: 'skiing', 995: 'advice', 996: 'allow', 997: 'tourist', 998: 'depressed', 999: 'intrigued', 1000: 'listening', 1001: 'resilient', 1002: 'unmarried', 1003: 'voting', 1004: 'helping', 1005: 'gets', 1006: 'simple', 1007: 'weapon', 1008: 'exciting', 1009: 'terrible', 1010: 'loud', 1011: 'proceed', 1012: 'choice', 1013: 'foot', 1014: 'ones', 1015: 'red', 1016: 'model', 1017: 'stir', 1018: 'shower', 1019: 'anyway', 1020: 'meant', 1021: 'fact', 1022: 'scary', 1023: 'doctors', 1024: 'sinking', 1025: 'useless', 1026: 'which', 1027: 'youd', 1028: 'ball', 1029: 'blame', 1030: 'empty', 1031: 'year', 1032: 'eaten', 1033: 'point', 1034: 'outgoing', 1035: 'sang', 1036: 'bike', 1037: 'brought', 1038: 'nauseous', 1039: 'guessed', 1040: 'flowers', 1041: 'lit', 1042: 'protest', 1043: 'travel', 1044: 'grateful', 1045: 'watching', 1046: 'rather', 1047: 'quitter', 1048: 'amused', 1049: 'untalented', 1050: 'bear', 1051: 'sounds', 1052: 'bargain', 1053: 'brand', 1054: 'ache', 1055: 'name', 1056: 'alright', 1057: 'evident', 1058: 'stopped', 1059: 'same', 1060: 'morons', 1061: 'chinese', 1062: 'ghost', 1063: 'glass', 1064: 'partner', 1065: 'dad', 1066: 'trouble', 1067: 'havent', 1068: 'laughing', 1069: 'hop', 1070: 'beats', 1071: 'tries', 1072: 'tidy', 1073: 'seated', 1074: 'fly', 1075: 'bark', 1076: 'excuse', 1077: 'hands', 1078: 'tripped', 1079: 'pass', 1080: 'cured', 1081: 'loyal', 1082: 'sober', 1083: 'snowed', 1084: 'thank', 1085: 'choose', 1086: 'stink', 1087: 'rush', 1088: 'swiss', 1089: 'doubt', 1090: 'art', 1091: 'listened', 1092: 'panicked', 1093: 'sat', 1094: 'baking', 1095: 'immune', 1096: 'rested', 1097: 'strict', 1098: 'thirty', 1099: 'wasted', 1100: 'cheap', 1101: 'phony', 1102: 'safer', 1103: 'pray', 1104: 'lighten', 1105: 'mama', 1106: 'alert', 1107: 'nap', 1108: 'sec', 1109: 'sunk', 1110: 'gloat', 1111: 'panic', 1112: 'chuckled', 1113: 'guts', 1114: 'squinted', 1115: 'hunk', 1116: 'nerd', 1117: 'monk', 1118: 'buried', 1119: 'class', 1120: 'dozed', 1121: 'exercised', 1122: 'cake', 1123: 'oppose', 1124: 'taxes', 1125: 'gas', 1126: 'surrender', 1127: 'washed', 1128: 'scream', 1129: 'finnish', 1130: 'baker', 1131: 'baffled', 1132: 'frantic', 1133: 'furious', 1134: 'snow', 1135: 'flat', 1136: 'monday', 1137: 'cloudy', 1138: 'urgent', 1139: 'warmer', 1140: 'split', 1141: 'means', 1142: 'plants', 1143: 'release', 1144: 'send', 1145: 'icky', 1146: 'drowned', 1147: 'evil', 1148: 'bore', 1149: 'realistic', 1150: 'beef', 1151: 'ramble', 1152: 'safely', 1153: 'ghosts', 1154: 'avoids', 1155: 'poet', 1156: 'kicked', 1157: 'hey', 1158: 'annoying', 1159: 'dare', 1160: 'horrible', 1161: 'jogging', 1162: 'chess', 1163: 'honey', 1164: 'women', 1165: 'jokes', 1166: 'cab', 1167: 'sugar', 1168: 'rewrote', 1169: 'coward', 1170: 'genius', 1171: 'surfer', 1172: 'cultured', 1173: 'freezing', 1174: 'grounded', 1175: 'homesick', 1176: 'involved', 1177: 'managing', 1178: 'rational', 1179: 'reformed', 1180: 'reliable', 1181: 'shopping', 1182: 'thinking', 1183: 'thrilled', 1184: 'ticklish', 1185: 'unbiased', 1186: 'iron', 1187: 'deer', 1188: 'cancer', 1189: 'white', 1190: 'burn', 1191: 'curse', 1192: 'bedtime', 1193: 'complex', 1194: 'missing', 1195: 'lead', 1196: 'goes', 1197: 'shrieked', 1198: 'forward', 1199: 'staring', 1200: 'whining', 1201: 'approve', 1202: 'crashed', 1203: 'theyll', 1204: 'succeeded', 1205: 'jobs', 1206: 'doomed', 1207: 'loser', 1208: 'vanished', 1209: 'scare', 1210: 'bossy', 1211: 'futon', 1212: 'included', 1213: 'talented', 1214: 'questions', 1215: 'reasonable', 1216: 'security', 1217: 'skate', 1218: 'favor', 1219: 'despair', 1220: 'peas', 1221: 'wins', 1222: 'coat', 1223: 'cookie', 1224: 'ham', 1225: 'cracked', 1226: 'hole', 1227: 'maid', 1228: 'braces', 1229: 'skating', 1230: 'pinched', 1231: 'stoned', 1232: 'demented', 1233: 'insecure', 1234: 'bled', 1235: 'thrilling', 1236: 'borrowed', 1237: 'bribed', 1238: 'caused', 1239: 'contributed', 1240: 'designed', 1241: 'queasy', 1242: 'reborn', 1243: 'finally', 1244: 'scammed', 1245: 'visa', 1246: 'orders', 1247: 'rights', 1248: 'camels', 1249: 'spring', 1250: 'sweets', 1251: 'trains', 1252: 'autumn', 1253: 'horses', 1254: 'summer', 1255: 'copy', 1256: 'insist', 1257: 'ought', 1258: 'rescheduled', 1259: 'respect', 1260: 'light', 1261: 'slapped', 1262: 'support', 1263: 'bath', 1264: 'pool', 1265: 'drugged', 1266: 'pleased', 1267: 'tempted', 1268: 'unarmed', 1269: 'behave', 1270: 'return', 1271: 'wore', 1272: 'brief', 1273: 'ambitious', 1274: 'concerned', 1275: 'contented', 1276: 'desperate', 1277: 'different', 1278: 'disgusted', 1279: 'impatient', 1280: 'important', 1281: 'impulsive', 1282: 'danger', 1283: 'motivated', 1284: 'powerless', 1285: 'surprised', 1286: 'terrified', 1287: 'uninsured', 1288: 'raining', 1289: 'worse', 1290: 'takes', 1291: 'test', 1292: 'superb', 1293: 'viral', 1294: 'gamble', 1295: 'relief', 1296: 'rental', 1297: 'possible', 1298: 'stealing', 1299: 'gambling', 1300: 'peace', 1301: 'bell', 1302: 'twin', 1303: 'beside', 1304: 'quietly', 1305: 'shouting', 1306: 'worrying', 1307: 'straighten', 1308: 'breath', 1309: 'chance', 1310: 'myth', 1311: 'asian', 1312: 'spies', 1313: 'japan', 1314: 'dvd', 1315: 'disagreed', 1316: 'hesitated', 1317: 'vulgar', 1318: 'second', 1319: 'before', 1320: 'sync', 1321: 'town', 1322: 'special', 1323: 'mail', 1324: 'snob', 1325: 'plumber', 1326: 'juggle', 1327: 'gate', 1328: 'recycle', 1329: 'crude', 1330: 'complain', 1331: 'judge', 1332: 'tease', 1333: 'everything', 1334: 'fishing', 1335: 'lip', 1336: 'dislikes', 1337: 'doesnt', 1338: 'uneasy', 1339: 'video', 1340: 'scolded', 1341: 'sold', 1342: 'panting', 1343: 'gambler', 1344: 'slacker', 1345: 'author', 1346: 'prison', 1347: 'spot', 1348: 'corrected', 1349: 'decorated', 1350: 'argue', 1351: 'flunk', 1352: 'dislike', 1353: 'gossip', 1354: 'strange', 1355: 'arrested', 1356: 'expelled', 1357: 'stroke', 1358: 'parties', 1359: 'secrets', 1360: 'spiders', 1361: 'sisters', 1362: 'voices', 1363: 'hiccup', 1364: 'hurried', 1365: 'picnics', 1366: 'puzzles', 1367: 'stories', 1368: 'keys', 1369: 'college', 1370: 'answers', 1371: 'raise', 1372: 'somebody', 1373: 'file', 1374: 'puppy', 1375: 'attacked', 1376: 'careless', 1377: 'detained', 1378: 'impolite', 1379: 'pardoned', 1380: 'glasses', 1381: 'direct', 1382: 'guide', 1383: 'musician', 1384: 'astonished', 1385: 'contagious', 1386: 'dehydrated', 1387: 'dependable', 1388: 'devastated', 1389: 'fascinated', 1390: 'firing', 1391: 'kyoto', 1392: 'interested', 1393: 'bitter', 1394: 'holiday', 1395: 'oldest', 1396: 'makes', 1397: 'wednesday', 1398: 'truth', 1399: 'searching', 1400: 'house', 1401: 'continue', 1402: 'prepare', 1403: 'prices', 1404: 'seal', 1405: 'adores', 1406: 'choked', 1407: 'roses', 1408: 'bach', 1409: 'teased', 1410: 'beauty', 1411: 'grumbling', 1412: 'wheel', 1413: 'bunch', 1414: 'tower', 1415: 'baby', 1416: 'train', 1417: 'theres', 1418: 'adore', 1419: 'bites', 1420: 'scam', 1421: 'road', 1422: 'cheered', 1423: 'blast', 1424: 'earlier', 1425: 'escaping', 1426: 'helpless', 1427: 'obedient', 1428: 'retiring', 1429: 'standing', 1430: 'disgust', 1431: 'screwed', 1432: 'amusing', 1433: 'callous', 1434: 'elusive', 1435: 'pro', 1436: 'capsized', 1437: 'adjust', 1438: 'present', 1439: 'insured', 1440: 'boil', 1441: 'afford', 1442: 'skip', 1443: 'mice', 1444: 'defend', 1445: 'says', 1446: 'interfere', 1447: 'interrupt', 1448: 'remind', 1449: 'rub', 1450: 'carefully', 1451: 'meal', 1452: 'prayed', 1453: 'everyones', 1454: 'mother', 1455: 'yet', 1456: 'hardly', 1457: 'painter', 1458: 'studied', 1459: 'freshman', 1460: 'slowpoke', 1461: 'southpaw', 1462: 'father', 1463: 'bill', 1464: 'interesting', 1465: 'appreciate', 1466: 'differ', 1467: 'breathe', 1468: 'chickened', 1469: 'couldnt', 1470: 'every', 1471: 'disobeyed', 1472: 'downloaded', 1473: 'expected', 1474: 'feverish', 1475: 'isolated', 1476: 'seizure', 1477: 'fanatics', 1478: 'bruise', 1479: 'evidence', 1480: 'showered', 1481: 'wed', 1482: 'match', 1483: 'wow', 1484: 'cheer', 1485: 'cuff', 1486: 'humor', 1487: 'aim', 1488: 'fantastic', 1489: 'shouted', 1490: 'timid', 1491: 'above', 1492: 'knits', 1493: 'wholl', 1494: 'wonderful', 1495: 'bottoms', 1496: 'carry', 1497: 'along', 1498: 'calls', 1499: 'fill', 1500: 'human', 1501: 'screamed', 1502: 'threw', 1503: 'accept', 1504: 'faster', 1505: 'flabby', 1506: 'humble', 1507: 'bogus', 1508: 'bulky', 1509: 'chat', 1510: 'comment', 1511: 'shadow', 1512: 'agrees', 1513: 'cheats', 1514: 'danced', 1515: 'drives', 1516: 'jumped', 1517: 'nodded', 1518: 'sighed', 1519: 'smokes', 1520: 'snores', 1521: 'yawned', 1522: 'cds', 1523: 'yelled', 1524: 'beers', 1525: 'shout', 1526: 'flip', 1527: 'coin', 1528: 'eight', 1529: 'nasty', 1530: 'resigned', 1531: 'slob', 1532: 'faking', 1533: 'taller', 1534: 'assume', 1535: 'jazz', 1536: 'math', 1537: 'rock', 1538: 'hide', 1539: 'glue', 1540: 'lips', 1541: 'humming', 1542: 'jittery', 1543: 'popular', 1544: 'puzzled', 1545: 'teasing', 1546: 'fatal', 1547: 'foggy', 1548: 'rain', 1549: 'yen', 1550: 'ironic', 1551: 'locked', 1552: 'poison', 1553: 'silent', 1554: 'lift', 1555: 'blushed', 1556: 'curt', 1557: 'tvs', 1558: 'blinked', 1559: 'coughed', 1560: 'inhaled', 1561: 'listens', 1562: 'hell', 1563: 'meet', 1564: 'share', 1565: 'drag', 1566: 'dump', 1567: 'heel', 1568: 'loss', 1569: 'whatd', 1570: 'drew', 1571: 'abandon', 1572: 'ship', 1573: 'litter', 1574: 'exist', 1575: 'spoon', 1576: 'another', 1577: 'heroic', 1578: 'mocked', 1579: 'romantic', 1580: 'caviar', 1581: 'easily', 1582: 'enjoyed', 1583: 'carded', 1584: 'soaked', 1585: 'beans', 1586: 'liars', 1587: 'hives', 1588: 'honor', 1589: 'improvised', 1590: 'china', 1591: 'r', 1592: 'b', 1593: 'opera', 1594: 'sushi', 1595: 'games', 1596: 'trips', 1597: 'space', 1598: 'piano', 1599: 'maybe', 1600: 'rose', 1601: 'suppose', 1602: 'sympathize', 1603: 'understood', 1604: 'facts', 1605: 'canned', 1606: 'framed', 1607: 'priest', 1608: 'purist', 1609: 'adult', 1610: 'agent', 1611: 'bleeding', 1612: 'eighteen', 1613: 'faithful', 1614: 'famished', 1615: 'fearless', 1616: 'gullible', 1617: 'hungover', 1618: 'rebel', 1619: 'offduty', 1620: 'restless', 1621: 'truthful', 1622: 'worn', 1623: 'checked', 1624: 'decided', 1625: 'elk', 1626: 'amazed', 1627: 'begun', 1628: 'noisy', 1629: 'heal', 1630: 'hideous', 1631: 'morning', 1632: 'suicide', 1633: 'ladies', 1634: 'bounce', 1635: 'hip', 1636: 'cared', 1637: 'filming', 1638: 'gawking', 1639: 'whiff', 1640: 'gold', 1641: 'huge', 1642: 'approves', 1643: 'spy', 1644: 'shrugged', 1645: 'stole', 1646: 'stutters', 1647: 'polite', 1648: 'unbelievable', 1649: 'arabs', 1650: 'starve', 1651: 'adults', 1652: 'closed', 1653: 'dating', 1654: 'shock', 1655: 'waste', 1656: 'woman', 1657: 'ink', 1658: 'years', 1659: 'amuse', 1660: 'gross', 1661: 'moody', 1662: 'mistaken', 1663: 'beware', 1664: 'body', 1665: 'drown', 1666: 'mock', 1667: 'sass', 1668: 'dies', 1669: 'haircut', 1670: 'few', 1671: 'dropped', 1672: 'dug', 1673: 'eats', 1674: 'british', 1675: 'toys', 1676: 'grouch', 1677: 'jesuit', 1678: 'tycoon', 1679: 'or', 1680: 'rope', 1681: 'runner', 1682: 'classes', 1683: 'part', 1684: 'snore', 1685: 'steal', 1686: 'poker', 1687: 'exaggerated', 1688: 'unwell', 1689: 'forbid', 1690: 'fought', 1691: 'bonus', 1692: 'celery', 1693: 'crowds', 1694: 'flying', 1695: 'tenure', 1696: 'sirens', 1697: 'people', 1698: 'things', 1699: 'cities', 1700: 'clocks', 1701: 'yellow', 1702: 'arabic', 1703: 'nature', 1704: 'poetry', 1705: 'mom', 1706: 'nearly', 1707: 'crew', 1708: 'lamp', 1709: 'yacht', 1710: 'violin', 1711: 'prefer', 1712: 'quite', 1713: 'resent', 1714: 'plane', 1715: 'queen', 1716: 'bacon', 1717: 'blood', 1718: 'firefox', 1719: 'crushed', 1720: 'mask', 1721: 'nights', 1722: 'poems', 1723: 'adaptable', 1724: 'orphan', 1725: 'available', 1726: 'convinced', 1727: 'delighted', 1728: 'easygoing', 1729: 'impressed', 1730: 'expert', 1731: 'obese', 1732: 'diet', 1733: 'parole', 1734: 'plastered', 1735: 'resentful', 1736: 'satisfied', 1737: 'saying', 1738: 'sensitive', 1739: 'surviving', 1740: 'damaged', 1741: 'painful', 1742: 'bat', 1743: 'apart', 1744: 'sickens', 1745: 'bomb', 1746: 'gift', 1747: 'hoax', 1748: 'parody', 1749: 'sequel', 1750: 'business', 1751: 'gorgeous', 1752: 'instinct', 1753: 'midnight', 1754: 'obsolete', 1755: 'occupied', 1756: 'pathetic', 1757: 'personal', 1758: 'suicidal', 1759: 'unlikely', 1760: 'unlocked', 1761: 'low', 1762: 'worth', 1763: 'i’m', 1764: 'falling', 1765: 'mortal', 1766: 'names', 1767: 'nobodys', 1768: 'russia', 1769: 'shake', 1770: 'active', 1771: 'gentle', 1772: 'obeys', 1773: 'livid', 1774: 'cutie', 1775: 'near', 1776: 'together', 1777: 'frowning', 1778: 'stuff', 1779: 'happens', 1780: 'suit', 1781: 'heap', 1782: 'saturn', 1783: 'tree', 1784: 'rad', 1785: 'untrue', 1786: 'embraced', 1787: 'italy', 1788: 'pun', 1789: 'wig', 1790: 'adopted', 1791: 'violent', 1792: 'wounded', 1793: 'snoring', 1794: 'focus', 1795: 'rules', 1796: 'tools', 1797: 'candy', 1798: 'rebuild', 1799: 'respond', 1800: 'succeed', 1801: 'group', 1802: 'anxious', 1803: 'cousins', 1804: 'stalled', 1805: 'waiting', 1806: 'winners', 1807: 'bummer', 1808: 'fiasco', 1809: 'hassle', 1810: 'ufo', 1811: 'whatll', 1812: 'coke', 1813: 'qualified', 1814: 'boats', 1815: 'sink', 1816: 'shovel', 1817: 'contact', 1818: 'grass', 1819: 'half', 1820: 'dinners', 1821: 'tigers', 1822: 'purr', 1823: 'matter', 1824: 'line', 1825: 'enter', 1826: 'gasps', 1827: 'form', 1828: 'lines', 1829: 'hint', 1830: 'park', 1831: 'afternoon', 1832: 'mug', 1833: 'birthday', 1834: 'holidays', 1835: 'cannot', 1836: 'counts', 1837: 'deceived', 1838: 'tan', 1839: 'flew', 1840: 'kite', 1841: 'gazed', 1842: 'beard', 1843: 'writer', 1844: 'actor', 1845: 'bankrupt', 1846: 'uncle', 1847: 'keeps', 1848: 'jelly', 1849: 'lives', 1850: 'owes', 1851: 'respects', 1852: 'excon', 1853: 'outlaw', 1854: 'henpecked', 1855: 'head', 1856: 'ached', 1857: 'end', 1858: 'high', 1859: 'hungarian', 1860: 'london', 1861: 'banana', 1862: 'borrow', 1863: 'leg', 1864: 'guarantee', 1865: 'vision', 1866: 'mondays', 1867: 'sundays', 1868: 'insects', 1869: 'ironing', 1870: 'karaoke', 1871: 'lawyers', 1872: 'spinach', 1873: 'ranch', 1874: 'table', 1875: 'truck', 1876: 'amnesia', 1877: 'hiccups', 1878: 'flu', 1879: 'diary', 1880: 'castles', 1881: 'history', 1882: 'mahjong', 1883: 'seafood', 1884: 'turtles', 1885: 'walking', 1886: 'oven', 1887: 'kobe', 1888: 'upstate', 1889: 'control', 1890: 'animals', 1891: 'bananas', 1892: 'lasagna', 1893: 'sunsets', 1894: 'sun', 1895: 'muffins', 1896: 'harm', 1897: 'decline', 1898: 'knife', 1899: 'snack', 1900: 'stamp', 1901: 'asap', 1902: 'surgery', 1903: 'painted', 1904: 'plead', 1905: 'predicted', 1906: 'cattle', 1907: 'realize', 1908: 'game', 1909: 'serve', 1910: 'sneeze', 1911: 'urge', 1912: 'caution', 1913: 'usually', 1914: 'chair', 1915: 'details', 1916: 'justice', 1917: 'revenge', 1918: 'member', 1919: 'barefoot', 1920: 'bluffing', 1921: 'captured', 1922: 'cleaning', 1923: 'hammered', 1924: 'outdoors', 1925: 'unharmed', 1926: 'wear', 1927: 'testify', 1928: 'cooperate', 1929: 'scold', 1930: 'minister', 1931: 'salesman', 1932: 'against', 1933: 'thumbs', 1934: 'behind', 1935: 'being', 1936: 'downstairs', 1937: 'exercising', 1938: 'farsighted', 1939: 'having', 1940: 'illiterate', 1941: 'making', 1942: 'meditating', 1943: 'methodical', 1944: 'racist', 1945: 'optimistic', 1946: 'prejudiced', 1947: 'remodeling', 1948: 'successful', 1949: 'killer', 1950: 'unemployed', 1951: 'vegetarian', 1952: 'been', 1953: 'forgotten', 1954: 'credible', 1955: 'likely', 1956: 'saturday', 1957: 'fluke', 1958: 'rumor', 1959: 'christmas', 1960: 'classic', 1961: 'mistake', 1962: 'ambush', 1963: 'beyond', 1964: 'dangerous', 1965: 'dishonest', 1966: 'expensive', 1967: 'forbidden', 1968: 'inspiring', 1969: 'fault', 1970: 'revolting', 1971: 'large', 1972: 'risky', 1973: 'undamaged', 1974: 'vibrating', 1975: 'lend', 1976: 'clues', 1977: 'effort', 1978: 'mums', 1979: 'joints', 1980: 'itches', 1981: 'tooth', 1982: 'tummy', 1983: 'tune', 1984: 'science', 1985: 'order', 1986: 'cooks', 1987: 'ditched', 1988: 'brains', 1989: 'nurse', 1990: 'awkward', 1991: 'sings', 1992: 'swims', 1993: 'types', 1994: 'hottie', 1995: 'looker', 1996: 'angel', 1997: 'reply', 1998: 'appeared', 1999: 'poking', 2000: 'resisting', 2001: 'thatll', 2002: 'illegal', 2003: 'logical', 2004: 'rubbish', 2005: 'barked', 2006: 'flag', 2007: 'melted', 2008: 'roof', 2009: 'leaks', 2010: 'their', 2011: 'snag', 2012: 'quarreled', 2013: 'babies', 2014: 'idiots', 2015: 'desk', 2016: 'absurd', 2017: 'doable', 2018: 'theirs', 2019: 'tricky', 2020: 'unsafe', 2021: 'throw', 2022: 'draws', 2023: 'crook', 2024: 'guest', 2025: 'moron', 2026: 'helpful', 2027: 'playing', 2028: 'trap', 2029: 'homeless', 2030: 'sweating', 2031: 'sauce', 2032: 'rear', 2033: 'crash', 2034: 'guests', 2035: 'landed', 2036: 'escape', 2037: 'action', 2038: 'heroes', 2039: 'shook', 2040: 'couple', 2041: 'family', 2042: 'brothers', 2043: 'partners', 2044: 'quitting', 2045: 'students', 2046: 'country', 2047: 'letdown', 2048: 'miracle', 2049: 'tragedy', 2050: 'causes', 2051: 'atm', 2052: 'would', 2053: 'wind', 2054: 'clock', 2055: 'wipe', 2056: 'balls', 2057: 'round', 2058: 'bees', 2059: 'anytime', 2060: 'police', 2061: 'whistle', 2062: 'boxes', 2063: 'dig', 2064: 'rap', 2065: 'freak', 2066: 'overdo', 2067: 'sheep', 2068: 'bags', 2069: 'trip', 2070: 'fasten', 2071: 'fetch', 2072: 'fold', 2073: 'bottom', 2074: 'acts', 2075: 'into', 2076: 'ambition', 2077: 'toyota', 2078: 'ten', 2079: 'intrigues', 2080: 'dreamer', 2081: 'gloomy', 2082: 'robot', 2083: 'laughs', 2084: 'puts', 2085: 'airs', 2086: 'whisky', 2087: 'stared', 2088: 'swindled', 2089: 'cheating', 2090: 'surfing', 2091: 'writes', 2092: 'comedian', 2093: 'frat', 2094: 'gardener', 2095: 'newcomer', 2096: 'husband', 2097: 'photogenic', 2098: 'mud', 2099: 'unfortunate', 2100: 'howd', 2101: 'hypnotism', 2102: 'abhor', 2103: 'admit', 2104: 'bachelor', 2105: 'egypt', 2106: 'spain', 2107: 'salad', 2108: 'baked', 2109: 'tongue', 2110: 'booked', 2111: 'compete', 2112: 'cooked', 2113: 'dealt', 2114: 'notice', 2115: 'dried', 2116: 'death', 2117: 'relieved', 2118: 'sunburned', 2119: 'tomatoes', 2120: 'resign', 2121: 'funerals', 2122: 'voice', 2123: 'politics', 2124: 'reptiles', 2125: 'rug', 2126: 'violence', 2127: 'weddings', 2128: 'laptop', 2129: 'sister', 2130: 'theory', 2131: 'alibi', 2132: 'diabetes', 2133: 'homework', 2134: 'immunity', 2135: 'fever', 2136: 'returned', 2137: 'noise', 2138: 'screams', 2139: 'rains', 2140: 'imagined', 2141: 'learned', 2142: 'lent', 2143: 'cartoons', 2144: 'swimming', 2145: 'teaching', 2146: 'westerns'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09x6r-05D4fM"
      },
      "source": [
        "def decode_seq(input_seq):\n",
        "  state_values = encoder_model.predict(input_seq)\n",
        "\n",
        "  target_seq = np.zeros((1,1))\n",
        "\n",
        "  target_seq[0,0] = decoder_word_index['start']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + state_values)\n",
        "\n",
        "    sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
        "    sampled_char = reverse_output_char_index[sampled_token_index]\n",
        "\n",
        "    decoded_sentence += ' ' + sampled_char\n",
        "\n",
        "    if(sampled_char == 'end' or len(decoded_sentence) > 52):\n",
        "      stop_condition = True\n",
        "\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = sampled_token_index\n",
        "\n",
        "    state_values = [h,c] \n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3QutY-1D_tH",
        "outputId": "7dcc7f0d-156a-4b0e-9d43-07e8d26b92d6"
      },
      "source": [
        "# testing the model for a sample from existing data\n",
        "for seq_index in [1234, 4356, 4565, 34, 2345, 7656]:\n",
        "  input_seq = encoder_input_data[seq_index:seq_index+1]\n",
        "  decoded_sentence = decode_seq(input_seq)\n",
        "  print('----')\n",
        "  print('Input_sentence: ', data.English[seq_index:seq_index+1])\n",
        "  print('French sentence: ', decoded_sentence)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            "Input_sentence:  1234    i wrote it\n",
            "Name: English, dtype: object\n",
            "French sentence:   <OOV> tomates tomates opération tomates plierai plierai\n",
            "----\n",
            "Input_sentence:  4356    youre funny\n",
            "Name: English, dtype: object\n",
            "French sentence:   dimbécile  imprudents lemportes attardés mourut mourut\n",
            "----\n",
            "Input_sentence:  4565    do you get it\n",
            "Name: English, dtype: object\n",
            "French sentence:   cancer maimes  sagissaitil détestes décroche bien  cinq\n",
            "----\n",
            "Input_sentence:  34    got it\n",
            "Name: English, dtype: object\n",
            "French sentence:   <OOV> vienstu magnifique  connus incroyables incroyables\n",
            "----\n",
            "Input_sentence:  2345    is tom well\n",
            "Name: English, dtype: object\n",
            "French sentence:   <OOV> parvienstu connait peuton fraternité fraternité\n",
            "----\n",
            "Input_sentence:  7656    im interested\n",
            "Name: English, dtype: object\n",
            "French sentence:   <OOV> approche plierai plierai plierai plierai plierai\n"
          ]
        }
      ]
    }
  ]
}